/**
 * Generated by orval v7.14.0 üç∫
 * Do not edit manually.
 * Generate Video API
 * Firefly Services Generate Video API allows you to use Firefly AI to generate videos from text.
 * OpenAPI spec version: 1.0.0
 */
export interface AnchorPromptsKey {
  anchor_prompt: string[];
}

/**
 * The error details within the error response.
 */
export interface ApiError {
  /** Indicates the type of error that occurred. */
  error_code: string;
  /** A human-readable error message. */
  message?: string;
  /** Stack trace of the error for debugging purposes. */
  stack_trace?: string[];
  /** Detailed validation error messages. */
  validation_errors?: ValidationErrorMessage[];
}

export interface AssetUploadResponse {
  /** @minItems 1 */
  assets: PublicBinaryOutput[];
}

export interface AsyncTaskLink {
  href: string;
}

export type AsyncTaskLinkType = (typeof AsyncTaskLinkType)[keyof typeof AsyncTaskLinkType];

// eslint-disable-next-line @typescript-eslint/no-redeclare
export const AsyncTaskLinkType = {
  cancel: 'cancel',
  result: 'result',
} as const;

export type AsyncTaskResponseLinks = { [key: string]: AsyncTaskLink };

export type AsyncTaskResponseProgress = number | null;

/**
 * Response type for async requests
 */
export interface AsyncTaskResponse {
  links: AsyncTaskResponseLinks;
  progress?: AsyncTaskResponseProgress;
}

export interface BodyGenExtendVideoV2 {
  /** Files to be processed */
  files: Blob[];
  request: ExtendVideoRequest;
}

export interface BodyGenExtendVideoV3 {
  /** Files to be processed */
  files: Blob[];
  request: ExtendVideoRequestV3;
}

export interface BodyGenerateVideoV2 {
  /** Files to be processed */
  files: Blob[];
  request: GenerateVideoRequest;
}

export interface BodyGenerateVideoV3 {
  /** Files to be processed */
  files: Blob[];
  request: GenerateVideoRequestV3;
}

export interface BodyPromptForgeFilixV2 {
  /** Files to be processed */
  files: Blob[];
  request: PromptForgeRequest;
}

export type CameraMotion = (typeof CameraMotion)[keyof typeof CameraMotion];

// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CameraMotion = {
  camera_pan_left: 'camera pan left',
  camera_pan_right: 'camera pan right',
  camera_zoom_in: 'camera zoom in',
  camera_zoom_out: 'camera zoom out',
  camera_tilt_up: 'camera tilt up',
  camera_tilt_down: 'camera tilt down',
  camera_locked_down: 'camera locked down',
  camera_handheld: 'camera handheld',
} as const;

export interface ClinetoSize {
  /**
   * The height of the output image.
   * @minimum 1
   * @maximum 8192
   */
  height: number;
  /**
   * The width of the output image.
   * @minimum 1
   * @maximum 8192
   */
  width: number;
}

export interface ClipRange {
  /** The time stamp of a clip end position, in seconds */
  end?: number;
  /** The time stamp of a clip start position, in seconds */
  start?: number;
}

export interface ControlData {
  structureData?: StructureReference;
}

export interface CreateSessionResponse {
  endpoint: string;
  sessionId: string;
  ttlSeconds: number;
}

export interface DebiasedPrompt {
  debiased_prompt: string;
  seed: number;
}

export interface DeleteSessionResponse {
  sessionId: string;
}

/**
 * the place of reference range and the target range
 */
export type ExtendVideoRequestPlacement = Placement | null;

/**
 * If specified alongside with n, the number of seeds must be the equal to n
 */
export type ExtendVideoRequestSeeds = number[] | null;

/**
 * The size of the requested generations
 */
export type ExtendVideoRequestSizes = ClinetoSize[] | null;

export interface ExtendVideoRequest {
  /** the constant rate factor for encoding video */
  bitRateFactor?: unknown;
  /** Model version to generate with. 'video1_8_standard' for the default model. */
  modelVersion?: VideoModelVersion;
  /** the place of reference range and the target range */
  placement?: ExtendVideoRequestPlacement;
  /** If specified alongside with n, the number of seeds must be the equal to n */
  seeds?: ExtendVideoRequestSeeds;
  /** The size of the requested generations */
  sizes?: ExtendVideoRequestSizes;
  video: InputVideo;
}

export interface ExtendVideoRequestV3 {
  /**
   * The constant rate factor for encoding video. Can be omitted/empty.
   * @minimum -1
   * @maximum 64
   */
  bitRateFactor?: number;
  /** The place of reference range and the target range. Can be omitted/empty. */
  placement?: Placement;
  /**
   * If specified alongside with n, the number of seeds must be the equal to n. Can be omitted/empty.
   * @minItems 1
   * @maxItems 4
   */
  seeds?: number[];
  /** The size of the requested generations. Can be omitted/empty. */
  sizes?: ClinetoSize[];
  video: InputVideoV3;
}

/**
 * the constant rate factor for encoding video
 */
export type GenerateVideoRequestBitRateFactor = number | null;

/**
 * The controls for the video
 */
export type GenerateVideoRequestControlData = ControlData | null;

/**
 * The image conditions for the video
 */
export type GenerateVideoRequestImage = InputImage | null;

/**
 * The locale will be used to generate content that is more relevant for user's country and language
 */
export type GenerateVideoRequestLocale = string | null;

/**
 * Inference will try to generate against this prompt
 */
export type GenerateVideoRequestNegativePrompt = string | null;

/**
 * The prompt used to generate the image. The longer the prompt - the better
 */
export type GenerateVideoRequestPrompt = string | null;

/**
 * If specified alongside with n, the number of seeds must be the equal to n
 */
export type GenerateVideoRequestSeeds = number[] | null;

/**
 * The size of the requested generations
 */
export type GenerateVideoRequestSizes = ClinetoSize[] | null;

export type GenerateVideoRequestVideoSettings = VideoSettings | null;

export interface GenerateVideoRequest {
  /** the constant rate factor for encoding video */
  bitRateFactor?: GenerateVideoRequestBitRateFactor;
  /** The controls for the video */
  controlData?: GenerateVideoRequestControlData;
  /** The image conditions for the video */
  image?: GenerateVideoRequestImage;
  /** The locale will be used to generate content that is more relevant for user's country and language */
  locale?: GenerateVideoRequestLocale;
  /** Model version to generate with. 'video1_8_standard' for the default model. */
  modelVersion?: VideoModelVersion;
  /** Inference will try to generate against this prompt */
  negativePrompt?: GenerateVideoRequestNegativePrompt;
  /** Output spec for the generated videos */
  output?: VideoOutputSpec;
  /** The prompt used to generate the image. The longer the prompt - the better */
  prompt?: GenerateVideoRequestPrompt;
  /** If specified alongside with n, the number of seeds must be the equal to n */
  seeds?: GenerateVideoRequestSeeds;
  /** The size of the requested generations */
  sizes?: GenerateVideoRequestSizes;
  videoSettings?: GenerateVideoRequestVideoSettings;
}

export interface GenerateVideoRequestV3 {
  /**
   * The constant rate factor for encoding video. 0 indicates a lossless generation, with the highest quality and largest file size. 63 indicates the worst quality generation with the smallest file size. The suggested value range is 17-23.
   * @minimum 0
   * @maximum 63
   */
  bitRateFactor?: number;
  /** The details of the image used as a keyframe for the generated video. Provided images are used as a first frame or final frame to guide the video generation. */
  image?: InputImageV3;
  /** The prompt used to generate the image. The longer the prompt, the better. */
  prompt?: string;
  /**
   * The seed reference value. Currently only 1 seed is supported.
   * @minItems 1
   * @maxItems 1
   */
  seeds?: number[];
  /** The dimensions of the generated video. Review the usage notes for [supported aspect ratios](/firefly-services/docs/firefly-api/guides/help/usage_notes/) and the sizes associated with them. */
  sizes?: ClinetoSize[];
  /** The camera and shot control settings. */
  videoSettings?: VideoSettingsV3;
}

export interface ImageCondition {
  /** The placement of the condition image */
  placement: PlacementStart;
  /** The image of the condition */
  source: PublicBinaryInput;
}

export interface ImageConditionV3 {
  /** Details about the timeline placement of the image. */
  placement: PlacementStart;
  /** The source details of the image. */
  source: PublicBinaryInputV3;
}

export interface InputImage {
  /** The image conditions for the video */
  conditions?: ImageCondition[];
}

export interface InputImageV3 {
  /** The details about the keyframe images used for the video generation. */
  conditions?: ImageConditionV3[];
}

export interface InputVideo {
  source: PublicBinaryInput;
}

export interface InputVideoV3 {
  source: PublicBinaryInputV3;
}

/**
 * the position of generated clip
 */
export type PlacementInset = ClipRange | null;

/**
 * the clip used as condition
 */
export type PlacementReference = ClipRange | null;

export interface Placement {
  /** the position of generated clip */
  inset?: PlacementInset;
  /** the clip used as condition */
  reference?: PlacementReference;
}

export interface PlacementStart {
  /**
   * The position of the image on the timeline for the generated video, 0 being the first frame and 1 being the last frame.
   * @minimum 0
   * @maximum 1
   */
  position: number;
}

export interface PromptForgeErrors {
  is_blocked_class_artist?: boolean;
  is_blocked_class_nsfw?: boolean;
  is_denied?: boolean;
  is_empty_prompt?: boolean;
  is_not_supported_language?: boolean;
  is_silent?: boolean;
}

export interface PromptForgeOutput {
  debiased_prompt: string;
  debiased_prompts?: DebiasedPrompt[];
  expanded_prompts?: string[];
  output_prompt: string;
}

export type PromptForgePipeline = (typeof PromptForgePipeline)[keyof typeof PromptForgePipeline];

// eslint-disable-next-line @typescript-eslint/no-redeclare
export const PromptForgePipeline = {
  CME: 'CME',
  CLIO: 'CLIO',
  CLIO_NO_NSFW: 'CLIO_NO_NSFW',
  DEV: 'DEV',
  PEOPLE_BANLIST: 'PEOPLE_BANLIST',
  NO_PEOPLE_BANLIST: 'NO_PEOPLE_BANLIST',
  AUTOCOMPLETE: 'AUTOCOMPLETE',
  NSFW_CLASSIFIER: 'NSFW_CLASSIFIER',
  NSFW_PG13_CLASSIFIER: 'NSFW_PG13_CLASSIFIER',
  T2V_PROMPT_REWRITE: 'T2V_PROMPT_REWRITE',
} as const;

export type PromptForgeRequestInputExpanded = string[] | null;

export interface PromptForgeRequest {
  age_debias_enbaled?: boolean;
  check_language?: boolean;
  debias_enabled?: boolean;
  debug_enabled?: boolean;
  expansion_enabled?: boolean;
  input_expanded?: PromptForgeRequestInputExpanded;
  input_tag_people?: boolean;
  lang: string;
  pipeline: PromptForgePipeline;
  prompt: string;
  seeds: number[];
}

export type PromptForgeResponseContainRenderingText = boolean[] | null;

export type PromptForgeResponseDebugOutput = string[] | null;

export type PromptForgeResponseReforgedPrompts = string[] | null;

export interface PromptForgeResponse {
  anchor_prompts: AnchorPromptsKey[];
  atoms_ran: string[][];
  contain_people: boolean[];
  contain_rendering_text?: PromptForgeResponseContainRenderingText;
  debug_output?: PromptForgeResponseDebugOutput;
  errors: PromptForgeErrors[];
  expandedOutput?: string[];
  language_detected: string[];
  output: PromptForgeOutput[];
  pipeline: PromptForgePipeline[];
  reforged_prompts?: PromptForgeResponseReforgedPrompts;
}

/**
 * Optional ID of the file in ACP. This has to be an ACP file assetID.
 */
export type PublicBinaryInputCreativeCloudFileId = string | null;

/**
 * This is coming from other outputs
 */
export type PublicBinaryInputId = string | null;

export type PublicBinaryInputName = string | null;

/**
 * This URL will expire in one hour
 */
export type PublicBinaryInputPresignedUrl = string | null;

/**
 * In public HTTP APIs by convention we use this object to represent binary data (image, mesh, ...);
even though for internal rquests we have different types.

Inputs (at least one should be provided):

- id: If this object represents a file in our s3 temp storage system, the ID for that. This is usually
  used when caller wants to send the image back to us.
- presigned_url: If the object is stored as pre-signed URL.
- creative_cloud_file_id: if the object is stored as as ACP file; id of the file.
- name: This is used if the object is included in the request or response payload as form. This field will
  be the name of the multi-part file. When accepting this file in the adapter, you don't need to use this field
  directly, instead the _source (binary) and _mime_type (string) will be set for you.
  When returning this type, tou don't need to set this field directly, instead set the bytes,
  and the mime type, and inference platform takes care of converting to multi-part.
    ```
    binary = PublicBinary(); binary._source = ...;  binary._mime_type = ...
    ```

It is also possible to set multiple fields; in case you want to return a file stored in various places
to the user.
 */
export interface PublicBinaryInput {
  /** Optional ID of the file in ACP. This has to be an ACP file assetID. */
  creativeCloudFileId?: PublicBinaryInputCreativeCloudFileId;
  /** This is coming from other outputs */
  id?: PublicBinaryInputId;
  name?: PublicBinaryInputName;
  /** This URL will expire in one hour */
  presignedUrl?: PublicBinaryInputPresignedUrl;
}

/**
 * Optional ID of the file in ACP. This has to be an ACP file assetID.
 */
export type PublicBinaryOutputCreativeCloudFileId = string | null;

/**
 * This is coming from other outputs
 */
export type PublicBinaryOutputId = string | null;

export type PublicBinaryOutputName = string | null;

/**
 * This URL will expire in one hour
 */
export type PublicBinaryOutputPresignedUrl = string | null;

/**
 * In public HTTP APIS by convention we use this object to represent binary data (image, mesh, ...);
even though for internal rquests we have different types.

Inputs (at least one should be provided):

- id: If this object represents a file in our s3 temp storage system, the ID for that. This is usually
  used when caller wants to send the image back to us.
- presigned_url: If the object is stored as pre-signed URL.
- creative_cloud_file_id: if the object is stored as as ACP file; id of the file.
- name: This is used if the object is included in the request or response payload as form. This field will
  be the name of the multi-part file. When accepting this file in the adapter, you don't need to use this field
  directly, instead the _source (binary) and _mime_type (string) will be set for you.
  When returning this type, tou don't need to set this field directly, instead set the bytes,
  and the mime type, and inference platform takes care of converting to multi-part.
    ```
    binary = PublicBinary(); binary._source = ...;  binary._mime_type = ...
    ```

It is also possible to set multiple fields; in case you want to return a file stored in various places
to the user.
 */
export interface PublicBinaryOutput {
  /** Optional ID of the file in ACP. This has to be an ACP file assetID. */
  creativeCloudFileId?: PublicBinaryOutputCreativeCloudFileId;
  /** This is coming from other outputs */
  id?: PublicBinaryOutputId;
  name?: PublicBinaryOutputName;
  /** This URL will expire in one hour */
  presignedUrl?: PublicBinaryOutputPresignedUrl;
}

export interface PublicBinaryInputV3 {
  /** The ID of the file in Adobe Creative Cloud. Has to be a Creative Cloud file asset ID. */
  creativeCloudFileId?: string;
  /** The internal ID for a storage item that is coming from other outputs. */
  uploadId?: string;
  /**
   * The pre-signed URL for the input file.
   * @minLength 1
   * @maxLength 4096
   */
  url?: string;
}

export type ShotAngle = (typeof ShotAngle)[keyof typeof ShotAngle];

// eslint-disable-next-line @typescript-eslint/no-redeclare
export const ShotAngle = {
  aerial_shot: 'aerial shot',
  eye_level_shot: 'eye_level shot',
  high_angle_shot: 'high angle shot',
  low_angle_shot: 'low angle shot',
  'top-down_shot': 'top-down shot',
} as const;

export type ShotSize = (typeof ShotSize)[keyof typeof ShotSize];

// eslint-disable-next-line @typescript-eslint/no-redeclare
export const ShotSize = {
  'close-up_shot': 'close-up shot',
  'extreme_close-up': 'extreme close-up',
  medium_shot: 'medium shot',
  long_shot: 'long shot',
  extreme_long_shot: 'extreme long shot',
} as const;

export interface StructureReference {
  depthStrength?: number;
  edgeStrength?: number;
  referenceVideo: InputVideo;
}

/**
 * Additional context for the validation error.
 */
export type ValidationErrorMessageCtx = { [key: string]: unknown };

export type ValidationErrorMessageLocItem = number | string;

export interface ValidationErrorMessage {
  /** Additional context for the validation error. */
  ctx?: ValidationErrorMessageCtx;
  /** Location of the validation error in the request. */
  loc: ValidationErrorMessageLocItem[];
  /** Validation error message. */
  msg: string;
  /** Type of validation error. */
  type: string;
}

export type VideoModelVersion = (typeof VideoModelVersion)[keyof typeof VideoModelVersion];

// eslint-disable-next-line @typescript-eslint/no-redeclare
export const VideoModelVersion = {
  video15: 'video1.5',
  video1_8_standard: 'video1_8_standard',
} as const;

export interface VideoOutputSpec {
  /** If `true`, Firefly and ACPC will store the output as a DCX composite which contains the input recipe information along with output content. If `false`, it will store it as plain content. */
  storeInputs?: boolean;
}

export type VideoPromptStyle = (typeof VideoPromptStyle)[keyof typeof VideoPromptStyle];

// eslint-disable-next-line @typescript-eslint/no-redeclare
export const VideoPromptStyle = {
  anime: 'anime',
  '3d': '3d',
  fantasy: 'fantasy',
  cinematic: 'cinematic',
  claymation: 'claymation',
  line_art: 'line art',
  stop_motion: 'stop motion',
  '2d': '2d',
  vector_art: 'vector art',
  black_and_white: 'black and white',
} as const;

export interface VideoSettings {
  /** The camera motion control */
  cameraMotion?: CameraMotion;
  /** The style of the generated video */
  promptStyle?: VideoPromptStyle;
  /** The shot angle control */
  shotAngle?: ShotAngle;
  /** The shot size control */
  shotSize?: ShotSize;
}

export interface VideoSettingsV3 {
  /** The camera motion control. */
  cameraMotion?: CameraMotion;
  /** The style of the generated video. */
  promptStyle?: VideoPromptStyle;
  /** The shot angle control. */
  shotAngle?: ShotAngle;
  /** The shot size control. */
  shotSize?: ShotSize;
}

export interface VideoResult {
  /** The pre-signed URL for the generated video file. */
  url: string;
}

export interface VideoOutput {
  /** The seed value used for generating this video output. */
  seed: number;
  video: VideoResult;
}

export interface AsyncResult {
  size: ClinetoSize;
  /** Array of generated video outputs. */
  outputs: VideoOutput[];
}

export interface AsyncResponseV3 {
  /** A URL to cancel the job. */
  cancelUrl?: unknown;
  /** The ID for the asyncronous job. */
  jobId: unknown;
  /** The progress of the running job. The value is the percentage of the job that has been completed. */
  progress?: number;
  /** The result of the completed job. */
  result?: AsyncResult;
  /** The status of the job. */
  status?: unknown;
  /** A URL to show the status of the current job. */
  statusUrl?: unknown;
}

/**
 * The headers of the message
 */
export type MessageResponseSchemaHeaders = { [key: string]: string };

/**
 * This is a schema for the final message response that is sent to the client after async job completed.
It is used in OpenAPI schema docs.
The actual response is of type starlette.types.Message
 */
export interface MessageResponseSchema {
  /** The body of the message */
  body: unknown;
  /** The headers of the message */
  headers: MessageResponseSchemaHeaders;
  /** The status code of the message */
  status: number;
  /** The status text of the message */
  status_text: string;
  /** The type of the message */
  type: string;
  /** The url of the message */
  url: string;
}

export type GenerateVideoV3202 = {
  jobId: string;
  statusUrl: string;
  cancelUrl: string;
};

export type JobResultV3PathParameters = {
  jobUrnOrId: string;
};
export type CancelJobV3PathParameters = {
  jobUrnOrId: string;
};
